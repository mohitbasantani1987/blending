{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\mohit\\\\Desktop\\\\analytics'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting datacleaning.py\n"
     ]
    }
   ],
   "source": [
    "%%file datacleaning.py\n",
    "\"\"\"\n",
    "This will clean the \n",
    "data handle outliers and give 5 bootstrap samples\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.stats as stats\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        \n",
    "def getBootstrapSample(df,Y):\n",
    "    \"\"\"\n",
    "    Take 2 input params and seperate cat and num data do outlier and missing value\n",
    "    treatment and return the 5 bootstrap sample.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    cat_col = seperate_cat_vars(df)\n",
    "    num_col = seperate_num_vars(df)\n",
    "    df_num=df[num_col]\n",
    "    missing_value_treatment=df_num.apply(lambda x: Missing_imputation(x))\n",
    "    outlier_treatment= df_num.apply(lambda x: outlier_capping(x))\n",
    "    df_cat=df[['department', 'salary']]\n",
    "    for c_feature in ['department', 'salary']:\n",
    "        df_cat[c_feature] = df_cat[c_feature].astype('category')\n",
    "        df_cat = create_dummies(df_cat , c_feature )\n",
    "    df_new = pd.concat([df_num, df_cat], axis=1)    \n",
    "    train ,test = train_test_split(df_new,test_size=0.3,random_state = 123 )\n",
    "    bs1 = train.sample(frac = 1, replace = True,random_state = 256)\n",
    "    bs2 = train.sample(frac = 1, replace = True,random_state = 257)\n",
    "    bs3 = train.sample(frac = 1, replace = True,random_state = 258)\n",
    "    bs4 = train.sample(frac = 1, replace = True,random_state = 259)\n",
    "    bs5 = train.sample(frac = 1, replace = True,random_state = 251)\n",
    "    data.append(bs1)\n",
    "    data.append(bs2)\n",
    "    data.append(bs3)\n",
    "    data.append(bs4)\n",
    "    data.append(bs5)\n",
    "    data.append(test)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def seperate_cat_vars(df):\n",
    "    cat_var_names = []\n",
    "    cat_var_names=[key for key in dict(df.dtypes) if dict(df.dtypes)[key] in ['object']]\n",
    "    return cat_var_names\n",
    "\n",
    "\n",
    "def seperate_num_vars(df):\n",
    "    numeric_var_names = []\n",
    "    numeric_var_names=[key for key in dict(df.dtypes) if dict(df.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\n",
    "    return numeric_var_names\n",
    "\n",
    "def Missing_imputation(x):\n",
    "    x = x.fillna(x.mean())\n",
    "    return x\n",
    "    \n",
    "def outlier_capping(x):\n",
    "    x = x.clip_upper(x.quantile(0.99))\n",
    "    x = x.clip_lower(x.quantile(0.01))\n",
    "    return x \n",
    "\n",
    "def create_dummies( df, colname ):\n",
    "    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n",
    "    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n",
    "    df = pd.concat([df, col_dummies], axis=1)\n",
    "    df.drop( colname, axis = 1, inplace = True )\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting decisiontree.py\n"
     ]
    }
   ],
   "source": [
    "%%file decisiontree.py\n",
    "\"\"\"\n",
    "this will build the 2 models in seperate functions using gini and entropy\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "import scipy.stats as stats\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.tree as dt\n",
    "import sklearn.ensemble as en\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        \n",
    "def getDecisionTreeByGini(bs):\n",
    "    \"\"\"\n",
    "    Take will take one bootstrap sample and return the model using gini\n",
    "    \"\"\"\n",
    "    bs_x = bs.columns.difference(['left'])\n",
    "    bs_x = bs[bs_x]\n",
    "    bs_y = bs.columns.difference(['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','department','salary'])\n",
    "    bs_y = bs[bs_y]\n",
    "    param_grid = {'max_depth': np.arange(3, 12),\n",
    "             'max_features': np.arange(3,8)}\n",
    "    tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 10)\n",
    "    tree.fit(bs_x,bs_y)\n",
    "    best_param = tree.best_params_\n",
    "    clf_tree = DecisionTreeClassifier( max_depth = best_param[\"max_depth\"], max_features=best_param[\"max_features\"] )\n",
    "    clf_tree.fit( bs_x, bs_y )\n",
    "    return clf_tree\n",
    "\n",
    "def getDecisionTreeByEntropy(bs):\n",
    "    \"\"\"\n",
    "    Take will take one bootstrap sample and return the model using entropy\n",
    "    \"\"\"\n",
    "    bs_x = bs.columns.difference(['left'])\n",
    "    bs_x = bs[bs_x]\n",
    "    bs_y = bs.columns.difference(['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','department','salary'])\n",
    "    bs_y = bs[bs_y]\n",
    "    param_grid = {'max_depth': np.arange(3, 12),\n",
    "             'max_features': np.arange(3,8) }\n",
    "    tree = GridSearchCV(DecisionTreeClassifier(criterion='entropy'),param_grid, cv = 10)\n",
    "    tree.fit(bs_x,bs_y)\n",
    "    best_param = tree.best_params_\n",
    "    clf_tree = DecisionTreeClassifier(max_depth = best_param[\"max_depth\"], max_features=best_param[\"max_features\"],criterion='entropy' )\n",
    "    clf_tree.fit( bs_x, bs_y )\n",
    "    return clf_tree\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting randomforesttree.py\n"
     ]
    }
   ],
   "source": [
    "%%file randomforesttree.py\n",
    "\"\"\"\n",
    "this will build the randomforest and will return the output\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree as dt\n",
    "import sklearn.ensemble as en\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        \n",
    "def getRandomForest(bs,decision_gini_model=None,decision_entropy_model=None,random_forest_model=None):\n",
    "    \n",
    "    if(decision_gini_model == None and decision_entropy_model == None and random_forest_model == None):\n",
    "        bs_x = bs.columns.difference(['left'])\n",
    "        bs_x = bs[bs_x]\n",
    "        bs_y = bs.columns.difference(['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','department','salary'])\n",
    "        bs_y = bs[bs_y]\n",
    "        radm_clf = RandomForestClassifier(oob_score=True,n_estimators=100 )\n",
    "        radm_clf.fit( bs_x, bs_y )\n",
    "        return radm_clf\n",
    "    else:\n",
    "        bs_x = bs.columns.difference(['left'])\n",
    "        bs_x = bs[bs_x]\n",
    "        bs_y = bs.columns.difference(['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','department','salary'])\n",
    "        bs_y = bs[bs_y]\n",
    "        decision_gini_prob = pd.DataFrame(decision_gini_model.predict(bs_x))\n",
    "        decision_entropy_prob = pd.DataFrame(decision_entropy_model.predict(bs_x))\n",
    "        random_forest_prob = pd.DataFrame(random_forest_model.predict(bs_x))\n",
    "        return random_forest_prob\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gradientboosting.py\n"
     ]
    }
   ],
   "source": [
    "%%file gradientboosting.py\n",
    "\"\"\"\n",
    "this will build the adaboosting and will return the output\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree as dt\n",
    "import sklearn.ensemble as en\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        \n",
    "def getGradientBoosting(bs):\n",
    "    \n",
    "    bs_x = bs.columns.difference(['left'])\n",
    "    bs_x = bs[bs_x]\n",
    "    bs_y = bs.columns.difference(['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','department','salary'])\n",
    "    bs_y = bs[bs_y]\n",
    "    pargrid_ada = {'n_estimators': [100, 200, 400, 600, 800],\n",
    "               'learning_rate': [10 ** x for x in range(-3, 3)]}\n",
    "    grad_ada = GridSearchCV(estimator=GradientBoostingClassifier(), \n",
    "                        param_grid=pargrid_ada, \n",
    "                        cv=5,\n",
    "                        verbose=True, n_jobs=-1)\n",
    "    grad_ada.fit(bs_x, bs_y)\n",
    "    return grad_ada\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bagging.py\n"
     ]
    }
   ],
   "source": [
    "%%file bagging.py\n",
    "\"\"\"\n",
    "this will build the adaboosting and will return the output\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.tree as dt\n",
    "import sklearn.ensemble as en\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, export\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "        \n",
    "        \n",
    "def getBaggingClassifier(bs):\n",
    "    \n",
    "    bs_x = bs.columns.difference(['left'])\n",
    "    bs_x = bs[bs_x]\n",
    "    bs_y = bs.columns.difference(['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','department','salary'])\n",
    "    bs_y = bs[bs_y]\n",
    "    bagclm = BaggingClassifier(oob_score=True, n_estimators=100)\n",
    "    bagclm.fit(bs_x, bs_y)\n",
    "    return bagclm\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
